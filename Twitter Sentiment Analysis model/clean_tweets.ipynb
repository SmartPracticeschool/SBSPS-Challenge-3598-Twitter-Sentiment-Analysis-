

def clean_text(txt):
    
   #clean text using regex
    txt=re.sub(r'@[A-Z0-9a-z_:]+','',txt) #replace username-tags
    txt=re.sub(r'^[RT]+','',txt)            #replace RT-tags
    txt = re.sub('https?://[A-Za-z0-9./]+','',txt)  #replace URLs
    txt=re.sub("[^a-zA-Z]", " ",txt)                #replace hashtags
    txt = txt.translate(str.maketrans('', '', string.punctuation)) # Remove punctuations

    # Remove stopwords
    txt_tokens = word_tokenize(txt)
    filtered_words = [words for words in tweet_tokens if not words in stop_words]
    
    ps = PorterStemmer()
    stemmed_words = [ps.stem(words) for words in filtered_words]
    lemmatizer = WordNetLemmatizer()
    lemma_words = [lemmatizer.lemmatize(w, pos='a') for words in stemmed_words]
    
